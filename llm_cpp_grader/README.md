This project is an ongoing LLM-based automated grading system designed to evaluate C++ programming assignments in a structured and objective manner. The system takes as input a problem description, a teacher reference solution, a grading rubric, and a student’s C++ code submission.

Using large language models and agent-based reasoning, the grading agent analyzes the student’s code, compares it with the reference solution, applies the rubric criteria, and generates a final numerical grade along with a detailed score breakdown and explanation. The output is produced in a structured JSON format to ensure consistency and easy integration with other systems.

The project explores different prompting and agentic strategies, including Chain-of-Thought reasoning, few-shot prompting, and evaluator-based workflows, and evaluates their effectiveness using a custom benchmark dataset with human-graded examples. A simple web interface is also included to demonstrate real-world usage of the grading system.

This project focuses on LLM reasoning, prompt engineering, AI agents, and practical AI system design for educational automation.